{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "izOOaVBj9qVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -U langchain openai tiktoken chromadb pypdf sentence_transformers langchain-openai"
      ],
      "metadata": {
        "id": "ecUfTB84fSkg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "ZFNlui_R9ECM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up the OpenAI key**"
      ],
      "metadata": {
        "id": "Wc0jGKgr9irB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EiUo6xXe8Zn0"
      },
      "outputs": [],
      "source": [
        "key = open(\"./data/OpenAI-key.txt\").read().split()[0]\n",
        "os.environ[\"OPENAI_API_KEY\"] = key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading PDF Files**"
      ],
      "metadata": {
        "id": "zSNfDVsa9ceX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdfLoader = DirectoryLoader(\"./data\",glob=\"./*.pdf\",loader_cls=PyPDFLoader)\n",
        "docs = pdfLoader.load()\n",
        "print(\"Total Documents Found: \",len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKpj_AEs9VDq",
        "outputId": "15a52055-a005-492b-8330-b12e7dcf7c5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents Found:  450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting text**"
      ],
      "metadata": {
        "id": "fIyHbazMUJYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size = 1000,\n",
        "                    chunk_overlap = 200,\n",
        "                    length_function = len\n",
        "                  )\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "_lJp74T9-gmz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1VHuVUDWkSc",
        "outputId": "bd9b2fd5-9bc0-4d8c-c4bb-ae718be66535"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2026"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing the vector store**"
      ],
      "metadata": {
        "id": "whBHmBljVE0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"ChromaDB\"\n",
        "embedding = OpenAIEmbeddings(disallowed_special=())\n",
        "db = Chroma.from_documents(\n",
        "                  documents = texts,\n",
        "                  embedding = embedding,\n",
        "                  persist_directory = dir\n",
        "                  )\n",
        "db.persist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU1F4W2NVLia",
        "outputId": "a0ff7635-9a22-4840-8e5d-6a12eba9f5b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load from the directory**"
      ],
      "metadata": {
        "id": "crU206vEZNv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = None\n",
        "db = Chroma(persist_directory = dir,\n",
        "            embedding_function = embedding\n",
        "            )"
      ],
      "metadata": {
        "id": "b15uvJ6BZM11"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Retreiver**"
      ],
      "metadata": {
        "id": "j3PteMJLZaAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()\n",
        "docs = retriever.get_relevant_documents(\"What is a prompt?\")"
      ],
      "metadata": {
        "id": "_FenUFcBZhnf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwieyk53ahP1",
        "outputId": "644a10c8-8ad3-4e20-fb11-ed179113ed65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='presented with a broad or undetailed prompt, its output predominantly exhibits a\\ngeneric nature, which, while being applicable across a range of contexts, may not be\\noptimal for any specific application. In contrast, a detailed and precise prompt enables\\nthe model to generate content that is more aligned with the unique requirements of\\n3', metadata={'page': 2, 'source': 'data/prompt engineering in LLM Review.pdf'}),\n",
              " Document(page_content='presented with a broad or undetailed prompt, its output predominantly exhibits a\\ngeneric nature, which, while being applicable across a range of contexts, may not be\\noptimal for any specific application. In contrast, a detailed and precise prompt enables\\nthe model to generate content that is more aligned with the unique requirements of\\n3', metadata={'page': 2, 'source': 'data/prompt engineering in LLM Review.pdf'}),\n",
              " Document(page_content='presented with a broad or undetailed prompt, its output predominantly exhibits a\\ngeneric nature, which, while being applicable across a range of contexts, may not be\\noptimal for any specific application. In contrast, a detailed and precise prompt enables\\nthe model to generate content that is more aligned with the unique requirements of\\n3', metadata={'page': 2, 'source': 'data/prompt engineering in LLM Review.pdf'}),\n",
              " Document(page_content='an appropriate is an interesting but challenging problem (Kumar and Talukdar, 2021).\\nPrompt Sharing All the above considerations refer to the application of prompt in a single task, domain or\\nlanguage. We may also consider prompt sharing , where prompt learning is applied to multiple tasks, domains, or\\nlanguages. Some key issues that may arise include how to design individual prompts for different tasks, and how to\\nmodulate their interaction with each other. So far this ﬁeld has not been explored. Fig.5 illustrates a simple multiple\\nprompt learning strategy for multiple tasks, where prompt templates are partially shared.\\n28', metadata={'page': 27, 'source': 'data/Survey on Prompt engineering.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating chain using Embeddings and retriever**"
      ],
      "metadata": {
        "id": "Jb1yGR50g6kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RetrievalQA.from_chain_type(\n",
        "                llm = OpenAI(),\n",
        "                chain_type = \"stuff\",\n",
        "                retriever = retriever,\n",
        "                return_source_documents = True\n",
        "              )"
      ],
      "metadata": {
        "id": "r9D9W5fsdNA0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVEuyDv5fk5L",
        "outputId": "c5b2f9cf-93ba-4e64-a658-9dc678657db2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7b09f58aa230>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7b09f72885e0>, openai_api_key='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', openai_proxy='')), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7b09f503afe0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outputs**"
      ],
      "metadata": {
        "id": "9fQ0grTVhBoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is prompt engineering?\"\n",
        "chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I57FjKFLfqEB",
        "outputId": "749e9096-eb10-43fc-9a9c-ba98ce07e9df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is prompt engineering?',\n",
              " 'result': ' Prompt engineering is the systematic design and optimization of input prompts to guide the responses of LLMs, ensuring accuracy, relevance, and coherence in the generated output. It is a crucial process in harnessing the full potential of LLMs and making them more applicable across diverse domains. Prompt engineering encompasses a spectrum of techniques, ranging from foundational approaches to more sophisticated methods, and is continually evolving through emerging research.',\n",
              " 'source_documents': [Document(page_content='behavior [15, 16].\\nThe discipline of prompt engineering has advanced alongside LLMs. What orig-\\ninated as a fundamental practice of shaping prompts to direct model outputs has\\nmatured into a structured research area, replete with its distinct methodologies and\\nestablished best practices. Prompt engineering refers to the systematic design and\\noptimization of input prompts to guide the responses of LLMs, ensuring accuracy, rel-\\nevance, and coherence in the generated output. This process is crucial in harnessing\\nthe full potential of these models, making them more accessible and applicable across\\ndiverse domains. Contemporary prompt engineering encompasses a spectrum of tech-\\nniques, ranging from foundational approaches such as role-prompting [17] to more\\nsophisticated methods such as “chain of thought” prompting [18]. The domain remains\\ndynamic, with emergent research continually unveiling novel techniques and applica-', metadata={'page': 1, 'source': 'data/prompt engineering in LLM Review.pdf'}),\n",
              "  Document(page_content='behavior [15, 16].\\nThe discipline of prompt engineering has advanced alongside LLMs. What orig-\\ninated as a fundamental practice of shaping prompts to direct model outputs has\\nmatured into a structured research area, replete with its distinct methodologies and\\nestablished best practices. Prompt engineering refers to the systematic design and\\noptimization of input prompts to guide the responses of LLMs, ensuring accuracy, rel-\\nevance, and coherence in the generated output. This process is crucial in harnessing\\nthe full potential of these models, making them more accessible and applicable across\\ndiverse domains. Contemporary prompt engineering encompasses a spectrum of tech-\\nniques, ranging from foundational approaches such as role-prompting [17] to more\\nsophisticated methods such as “chain of thought” prompting [18]. The domain remains\\ndynamic, with emergent research continually unveiling novel techniques and applica-', metadata={'page': 1, 'source': 'data/prompt engineering in LLM Review.pdf'}),\n",
              "  Document(page_content='behavior [15, 16].\\nThe discipline of prompt engineering has advanced alongside LLMs. What orig-\\ninated as a fundamental practice of shaping prompts to direct model outputs has\\nmatured into a structured research area, replete with its distinct methodologies and\\nestablished best practices. Prompt engineering refers to the systematic design and\\noptimization of input prompts to guide the responses of LLMs, ensuring accuracy, rel-\\nevance, and coherence in the generated output. This process is crucial in harnessing\\nthe full potential of these models, making them more accessible and applicable across\\ndiverse domains. Contemporary prompt engineering encompasses a spectrum of tech-\\nniques, ranging from foundational approaches such as role-prompting [17] to more\\nsophisticated methods such as “chain of thought” prompting [18]. The domain remains\\ndynamic, with emergent research continually unveiling novel techniques and applica-', metadata={'page': 1, 'source': 'data/prompt engineering in LLM Review.pdf'}),\n",
              "  Document(page_content='be reformulated a generation problems by providing appropriate prompts. Therefore, prompting methods (i)\\nbroaden the applicability of these generation-oriented pre-trained models. For example, pre-trained models\\nlike BART are less used in NER while prompting methods make BART applicable, and (ii) breaks the\\ndifﬁculty of uniﬁed modelling among different tasks (Khashabi et al., 2020).\\n4 Prompt Engineering\\nPrompt engineering is the process of creating a prompting function fprompt (x)that results in the most effective\\nperformance on the downstream task. In many previous works, this has involved prompt template engineering ,\\nwhere a human engineer or algorithm searches for the best template for each task the model is expected to perform.\\nAs shown in the “Prompt Engineering” section of Fig.1, one must ﬁrst consider the prompt shape , and then decide\\nwhether to take a manual orautomated approach to create prompts of the desired shape, as detailed below.\\n4.1 Prompt Shape', metadata={'page': 10, 'source': 'data/Survey on Prompt engineering.pdf'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why do we need prompts?\"\n",
        "chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTlN4u4PgITh",
        "outputId": "ec15e805-6d85-4fbc-a1ac-794ef74b557f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Why do we need prompts?',\n",
              " 'result': ' Prompts are necessary in order to guide the model towards solving specific tasks, such as text classification and generation, in natural language processing. They provide structured information to the model, making it easier to extract task-specific information and improve performance on downstream tasks. Additionally, prompts can be reformulated to suit different tasks or models, increasing their transferability and usefulness.',\n",
              " 'source_documents': [Document(page_content='10 Challenges\\nAlthough prompt-based learning has shown signiﬁcant potential among different tasks and scenarios, several\\nchallenges remain, some of which we detail below.\\n10.1 Prompt Design\\nTasks beyond Classiﬁcation and Generation Most existing works about prompt-based learning revolve around\\neither text classiﬁcation or generation-based tasks. Applications to information extraction and text analysis tasks\\nhave been discussed less, largely because the design of prompts is less straightforward. We expect that applying\\nprompting methods to these tasks in the future it will require either reformulating these tasks so that they can\\nbe solved using classiﬁcation or text generation-based methods, or performing effective answer engineering that\\nexpresses structured outputs in an appropriate textual format.\\nPrompting with Structured Information In many NLP tasks, the inputs are imbued with some variety of', metadata={'page': 26, 'source': 'data/Survey on Prompt engineering.pdf'}),\n",
              "  Document(page_content='10 Challenges\\nAlthough prompt-based learning has shown signiﬁcant potential among different tasks and scenarios, several\\nchallenges remain, some of which we detail below.\\n10.1 Prompt Design\\nTasks beyond Classiﬁcation and Generation Most existing works about prompt-based learning revolve around\\neither text classiﬁcation or generation-based tasks. Applications to information extraction and text analysis tasks\\nhave been discussed less, largely because the design of prompts is less straightforward. We expect that applying\\nprompting methods to these tasks in the future it will require either reformulating these tasks so that they can\\nbe solved using classiﬁcation or text generation-based methods, or performing effective answer engineering that\\nexpresses structured outputs in an appropriate textual format.\\nPrompting with Structured Information In many NLP tasks, the inputs are imbued with some variety of', metadata={'page': 26, 'source': 'data/Survey on Prompt engineering.pdf'}),\n",
              "  Document(page_content='10 Challenges\\nAlthough prompt-based learning has shown signiﬁcant potential among different tasks and scenarios, several\\nchallenges remain, some of which we detail below.\\n10.1 Prompt Design\\nTasks beyond Classiﬁcation and Generation Most existing works about prompt-based learning revolve around\\neither text classiﬁcation or generation-based tasks. Applications to information extraction and text analysis tasks\\nhave been discussed less, largely because the design of prompts is less straightforward. We expect that applying\\nprompting methods to these tasks in the future it will require either reformulating these tasks so that they can\\nbe solved using classiﬁcation or text generation-based methods, or performing effective answer engineering that\\nexpresses structured outputs in an appropriate textual format.\\nPrompting with Structured Information In many NLP tasks, the inputs are imbued with some variety of', metadata={'page': 26, 'source': 'data/Survey on Prompt engineering.pdf'}),\n",
              "  Document(page_content='labels of the downstream task.), making it easier to extract task-speciﬁc information. Saunshi et al. (2021) veriﬁed\\nthat text classiﬁcation tasks can be reformulated as sentence completion tasks, thus making language modeling a\\nmeaningful pre-training task. Scao and Rush (2021) empirically show that prompting is often worth 100s of data\\npoints on average across classiﬁcation tasks.\\n10.7 Transferability of Prompts\\nUnderstanding the extent to which prompts are speciﬁc to the model and improving the transferability of prompts are\\nalso important topics. (Perez et al., 2021) show that prompts selected under tuned few-shot learning scenario (where\\none has a larger validation set to choose prompts) generalize well across models of similar sizes while prompts\\nselected under true few-shot learning scenario (where one only has a few training samples) do not generalize as\\neffectively as the former setting among models with similar sizes. The transferability is poor when the model sizes', metadata={'page': 28, 'source': 'data/Survey on Prompt engineering.pdf'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How do LLMs work?\"\n",
        "chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az09X3IwgQSr",
        "outputId": "23fe9858-0afd-49e4-c89a-70c6f5536797"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'How do LLMs work?',\n",
              " 'result': ' LLMs (Language Model Models) are trained using large amounts of data, and they can be fine-tuned for specific tasks to improve performance. There are different approaches to sharing trained LLM models, including freely sharing them with a license, requiring individuals to apply for access, providing API access without model parameters or detailed information, or providing no access at all. These decisions are based on factors such as model use, potential harms, and business decisions. LLMs are susceptible to data leakage attacks, where significant segments of text can be extracted from the model.',\n",
              " 'source_documents': [Document(page_content='training logs will provide a guide for those training their own LLMs.\\nWe have several interesting directions to pursue. First, task ﬁne-tuning has yielded\\nsigniﬁcant improvements in LLMs, and we plan to consider what unique opportunities exist\\n38', metadata={'page': 37, 'source': 'data/BloombergGPT.pdf'}),\n",
              "  Document(page_content='training logs will provide a guide for those training their own LLMs.\\nWe have several interesting directions to pursue. First, task ﬁne-tuning has yielded\\nsigniﬁcant improvements in LLMs, and we plan to consider what unique opportunities exist\\n38', metadata={'page': 37, 'source': 'data/BloombergGPT.pdf'}),\n",
              "  Document(page_content='training logs will provide a guide for those training their own LLMs.\\nWe have several interesting directions to pursue. First, task ﬁne-tuning has yielded\\nsigniﬁcant improvements in LLMs, and we plan to consider what unique opportunities exist\\n38', metadata={'page': 37, 'source': 'data/BloombergGPT.pdf'}),\n",
              "  Document(page_content='of LLMs. One strategy is to freely and openly share trained models (Scao et al., 2022), and\\nrely on a license that dictates how the model should and should not be used. Another\\nrequires individuals to apply for access to the trained model parameters (Zhang et al.,\\n2022a; Touvron et al., 2023). A more restrictive approach is to provide API access to\\nmodels, but no access to the underlying model parameters or detailed information on the\\ndata the model was trained on (Brown et al., 2020). Finally, some have provided no access\\nto the model (Chowdhery et al., 2022; Hoﬀmann et al., 2022). Each decision reﬂects a\\ncombination of factors, including model use, potential harms, and business decisions.\\nOne of Bloomberg’s core business propositions is around providing access to data that\\nhas been collected over the course of decades. As is well known, LLMs are susceptible to\\ndata leakage attacks and it is possible to extract signiﬁcant segments of text given model', metadata={'page': 37, 'source': 'data/BloombergGPT.pdf'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}